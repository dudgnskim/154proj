---
title: "HRC Project"
author: "Dotun Fatade"
date: "November 4, 2016"
output: pdf_document
---

# 0. Install and Require Packages to be Used
```{r, message=FALSE,results='hide'}
pkg = c("ggplot2","SnowballC","tm","stringi", "stringr","rJava","openNLP", "NLP", "qdapDictionaries", "dplyr", "foreach", "doParallel", "doSNOW", "xgboost", "randomForest", "reshape2", "e1071")
new.pkg = pkg[!(pkg %in% installed.packages()[,"Package"])]
if (length(new.pkg)) {install.packages(new.pkg,dependencies = TRUE)}
sapply(pkg,require,character.only = TRUE)
```

# 1. Import Data
```{r}
# Log the script's output and messages to a text file.
sink(paste0(gsub("\\.[^.]+$", "", basename(sys.frame(1)$ofile)), ".log"), append=F, split=T)
cat("Executing:", sys.frame(1)$ofile, "\nDatetime:", date(), "\n")

# Start timing the script.
script_timer = proc.time()

# Load our main functions.
source("../part0/functionlib.Rmd")

# Directory to load
directory = "code/part0"
```


# 2. Feature Creation
```{r}
# Log the script's output and messages to a text file.
sink(paste0(gsub("\\.[^.]+$", "", basename(sys.frame(1)$ofile)), ".log"), append=F, split=T)
cat("Executing:", sys.frame(1)$ofile, "\nDatetime:", date(), "\n")

# Start timing the script.
script_timer = proc.time()

# --- End prelude.
#########################################

library(tm)
library(SnowballC) # for stemming
library(doMC) # For multicore processing.
# Setup multicore processing to speed up script execution.
load("data/imported_text_docs.RData")
stopwords = load_stopwords()

# Check if it's the same list as the one used by tm()
length(stopwords)
length(stopwords("english"))

# If not all of the official stopwords are in the tm list, we evaluate this one.
mean(stopwords %in% stopwords("english"))

cat("Cores detected:", detectCores(), "\n")
if (exists("conf")) {
  registerDoMC(conf$num_cores)
} else {
  # Uses half of the available cores by default, which is a good default setting.
  # On an Intel CPU that will correspond to the actual number of physical CPU cores.
  registerDoMC()
}
getDoParWorkers()

# Takes about 10 minutes; this does use multicore.
system.time({
  result = clean_imported_documents(docs, stopwords)
})

docs = result$docs
targets = result$targets

print(dim(docs))

# Double-check the results.
print(table(targets))
print(length(targets))

save(docs, targets, file="data/cleaned_docs.RData")

rm(result, docs, targets, stopwords)

#########################################
# Cleanup

gc()

# Review script execution time.
if (exists("script_timer")) {
  cat("Script execution time:", round((proc.time() - script_timer)[3] / 60, 0), "minutes.\n")
  rm(script_timer)
}

# Stop logging.
sink()
```

# 3. Features Filtering
```{r}
# Log the script's output and messages to a text file.
sink(paste0(gsub("\\.[^.]+$", "", basename(sys.frame(1)$ofile)), ".log"), append=F, split=T)
cat("Executing:", sys.frame(1)$ofile, "\nDatetime:", date(), "\n")

# Start timing the script.
script_timer = proc.time()

# --- End prelude.
#########################################

# Load the docs file, even if it already exists because it may the new filtered version.
load("data/cleaned_docs.RData")

# Remove words that are used in at least 100% of documents - 0 words.
cutoff_high_pct = 1

# Remove words that are not in at least 50 documents.
cutoff_low = 50

# Here we use the sum of how many times a word is used, which can be greater than 1 in a given doc.
# This is not exactly what the final-project PDF wants.
#system.time({
#  word_usage = apply(docs, MARGIN=2, FUN=sum)
#})

# Here we check if the value is not NA and greater than 0.
# This is basically a count of how many documents use the word.
# Takes 42+ seconds to run.
system.time({
  word_usage = apply(docs, MARGIN=2, FUN=function(x){ sum(!is.na(x) & x > 0) })
})
summary(word_usage)
hist(word_usage, breaks=30, main="Documents using the word")
dev.copy(png, "visuals/3_feature_filtering_histogram.png")
dev.off()

# How many words do we have right now?
ncol(docs)

cutoff_high = round(nrow(docs) * cutoff_high_pct)
cutoff_high
sum(word_usage > cutoff_high)
word_usage[word_usage > cutoff_high]
# Remove words that are above the cutoff.
docs = docs[, !colnames(docs) %in% names(word_usage[word_usage > cutoff_high]) ]

sum(word_usage < cutoff_low)
word_usage[word_usage < cutoff_low]
# Remove words that are above the cutoff.
docs = docs[, !colnames(docs) %in% names(word_usage[word_usage < cutoff_low]) ]

# Update word usage to reflect the revised features.
word_usage  = word_usage[word_usage >= cutoff_low & word_usage <= cutoff_high]
summary(word_usage)
hist(word_usage, breaks=30, main="Documents using the word (pruned)")
dev.copy(png, "visuals/3_feature_filtering_histogram_pruned.png")
dev.off()

# Final dimensions.
dim(docs)

# Save the result.
save(docs, targets, file="data/filtered_docs.RData")

# Clean up environment.
rm(cutoff_high, cutoff_low, word_usage, cutoff_high_pct, targets, docs)

#########################################
# Epilogue -- 

gc()

# Review script execution time.
if (exists("script_timer")) {
  cat("Script execution time:", round((proc.time() - script_timer)[3] / 60, 0), "minutes.\n")
  rm(script_timer)
}

# Stop logging.
sink()
```

# 4. Power Features
```{r}
# Log the script's output and messages to a text file.
sink(paste0(gsub("\\.[^.]+$", "", basename(sys.frame(1)$ofile)), ".log"), append=F, split=T)
cat("Executing:", sys.frame(1)$ofile, "\nDatetime:", date(), "\n")

# Start timing the script.
script_timer = proc.time()

# --- End prelude.
#########################################

library(tm)
library(SnowballC)
library(stringi)
library(stringr)
library(openNLP)
library(NLP)
library(qdap)
library(dplyr)
library(foreach)
library(doMC) # Setup multicore processing to speed up the processing.


# Remember to load the .Rproj file in Rstudio so that paths are relative to the project directory.
load("data/imported_text_docs.RData")

source("../part0/functionlib.Rmd")


cat("Cores detected:", detectCores(), "\n")
if (exists("conf")) {
  registerDoMC(conf$num_cores)
} else {
  # Uses half of the available cores by default, which is a good default setting.
  registerDoMC()
}
getDoParWorkers()

# Merge docs into a single object to compute ngrams on the full dataset.
combined_docs = do.call(c, docs)
length(combined_docs)
class(combined_docs)


stopwords = load_stopwords()

# This takes 29 minutes on my laptop, single core.
system.time({
  bigram_features = power_features_ngrams(combined_docs, stopwords, ngrams = 2)
})

dim(bigram_features)

# Check for NAs.
sum(apply(bigram_features, FUN=function(x){sum(is.na(x))}, MARGIN=2))

# Identify how many docs use each n-gram.
bigram_usage = apply(bigram_features, MARGIN=2, FUN=function(x){ sum(!is.na(x) & x > 0) })

# Look at the top 50 bigrams
sort(bigram_usage, decreasing=T)[1:50]


# This takes 29 minutes on my laptop, single core.
# TODO: many of these trigrams are from gutenberg legal disclaimer text - should put as stopwords or otherwise remove full disclaimer.
system.time({
  trigram_features = power_features_ngrams(combined_docs, stopwords, ngrams = 3)
})

dim(trigram_features)

# Check for NAs.
sum(apply(trigram_features, FUN=function(x){sum(is.na(x))}, MARGIN=2))

# Identify how many docs use each n-gram.
trigram_usage = apply(trigram_features, MARGIN=2, FUN=function(x){ sum(!is.na(x) & x > 0) })

# Look at the top 50 bigrams
sort(trigram_usage, decreasing=T)[1:50]

# TODO: remove some of the Gutenberg-focused trigrams so that they aren't extra noise during training.

# Save these features since they take a long time to calculate and R could crash.
save(bigram_features, trigram_features, file="data/ngram-power-features.Rdata")

# Use foreach here so that it can be run on multiple cores.
# This takes about 58 minutes to execute without multicore processing.
# TODO: get multicore processing to work.
system.time({
  #feature_list = foreach(worker = 1:getDoParWorkers(), .combine=rbind) %do% {
  sentence_features = foreach(worker = 1:length(names(docs)), .combine = "rbind") %do% {
    #power_features[[type]] = power_features_sentence(docs[[type]])
    result = power_features_sentence(docs[[worker]])
    result
  }
})

# Confirm that we created the sentence features successfully.
stopifnot(class(sentence_features) != "NULL")

dim(sentence_features)
# TODO: confirm that we get the results in the exactly correct order.

# Review summmary stats on sentence features.
apply(sentence_features, FUN=summary, MARGIN=2)

# Check for NAs.
apply(sentence_features, FUN=function(x){sum(is.na(x))}, MARGIN=2)

# Fix any NAs in the average sentence length (currently there is 1).
# TODO: remove the generation of NAs by fixing the sentence feature function.
sentence_features[is.na(sentence_features[, "sentence_avg_length"]), "sentence_avg_length"] = 0


# Again, check for NAs.
apply(sentence_features, FUN=function(x){sum(is.na(x))}, MARGIN=2)

# Save these features since they take a long time to calculate and R could crash.
save(sentence_features, file="data/sentence_power_features.RData")

load("data/cleaned_docs.RData")

# Run the dtm power features on the word feature dataframe.
# This takes ~3.6 hours to finish on EC2.
system.time({
  word_features = power_features_dtm(docs)
})

# Review summmary stats on word features.
apply(word_features, FUN=summary, MARGIN=2)

# Check for NAs.
apply(word_features, FUN=function(x){sum(is.na(x))}, MARGIN=2)

# Save these features since they take a long time to calculate and R could crash.
save(word_features, file="data/word_power_features.RData")

# Save component so that they can be analyzed more easily later.
save(sentence_features, word_features, bigram_features, trigram_features, file="data/power_feature_components.RData")

# Combine the sentence and word power features.
# TODO: need to make sure that we are combining in the correct order.
# Otherwise we need to merge on the book name/id
power_features = cbind(sentence_features, word_features, bigram_features, trigram_features)

# Check for NAs.

dim(power_features)

save(power_features, file="data/power_features.RData")


# Now load the filtered word features to create the combined feature matrix.
load("data/filtered_docs.Rdata")

combined_features = cbind(docs, power_features)

dim(combined_features)

save(combined_features, file="data/combined_features.RData")

# Cleanup objects
rm(docs, combined_docs, combined_features, power_features, bigram_features, trigram_features)
rm(bigram_usage, trigram_usage, word_features)
```

# 5. Review
```{r}
gc()

# Review script execution time.
if (exists("script_timer")) {
  cat("Script execution time:\n")
  print(proc.time() - script_timer)
  rm(script_timer)
}

# Stop logging.
sink()
```
