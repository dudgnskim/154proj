###Install Packages
```{r}
#Install Packages
pkg = c("ggplot2","tm", "SnowballC", "stringi", "stringr", "openNLP", "NLP", "qdap", "dplyr", "foreach", "XML")
new.pkg = pkg[!(pkg %in% installed.packages()[,"Package"])]
if (length(new.pkg)) {install.packages(new.pkg,dependencies = TRUE)}
sapply(pkg,require,character.only = TRUE)
```

###Load the file
```{r}
HRC = Corpus(DirSource('../../data/train', pattern = '.tsv'))
```

###Stopwords vector
```{r}
#Load stopwords downloaded from online sources into R as a vector
stopwords_from_online = scan("../../data/common-english-words.txt", what = "character", sep = ",")

#Load stopwords from builtin in the "tm" package
stopwords_builtin = stopwords()

#Append the stopwords that were found online and that were builtin and remove repeated words; store the new vector into the variable "stopwords"
stopwords = unique(append(stopwords_from_online, stopwords_builtin))
rm(stopwords_builtin, stopwords_from_online)
```

###Remove unnecessary stuff and change all to lower case
```{r}
clean = function(list) {
  remove_extra = function(x) {
    # Removing multiple punctuations, numbers, single/two-lettered words and whitespaces. (In   that order)
    x <- gsub("[[:punct:]]", "", x) # Removes punctuations
    x <- gsub("[[:digit:]]", "", x) # Removes numbers
    x <- gsub("*\\b[[:alpha:]]{1,2}\\b *", " ", x) # Removes any words with 1 or 2 letters
    #x <- gsub("\\b(\\S+?)\\1\\S*\\b", "", x, perl = TRUE) # Removes any words with repeating letters
    x <- gsub("\\s+", " ", x) # Removes multiple whitespaces.
    x <- trimws(x) # Remove leading and end whitespaces.
    return(x)
  }
  exclude_redundancy = function(x) {
    # Create a character vector that consists of meaningless word chunks.
    redun <- c("unclassified u.s. department of state case no.", "doc no.", "date:",
             "state dept. - produced to house select benghazi comm. subject to agreement on sensitive information & redactions.", "no foia waiver.", "subject:", "sent:", "monday",
               "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "f-",
               "release in full", "release in part", "state-", "subject", "fvv", "sent", "scb",
             "/", ":", ",")
    # Remove elements in redun from the email set
    x <- tolower(x)
    for (red in redun) {
    x <- gsub(red, "", x)
    }
    # Since we don't need any words more than once.
    x <- toString(unique(unlist(strsplit(x, split = " ", fixed = T))))
    return(x)
  }
  list <- unlist(lapply(lapply(lapply(list, exclude_redundancy), remove_extra), removeWords, stopwords))
  return(gsub("\\s+", " ",list))
}

clean_pw = function(list) {
  remove_extra = function(x) {
    # Removing multiple punctuations, numbers, single/two-lettered words and whitespaces. (In   that order)
    #x <- gsub("[[:punct:]]", "", x) # Removes punctuations
    x <- gsub("[[:digit:]]", "", x) # Removes numbers
    x <- gsub("*\\b[[:alpha:]]{1,2}\\b *", " ", x) # Removes any words with 1 or 2 letters
    #x <- gsub("\\b(\\S+?)\\1\\S*\\b", "", x, perl = TRUE) # Removes any words with repeating letters
    x <- gsub("\\s+", " ", x) # Removes multiple whitespaces.
    x <- trimws(x) # Remove leading and end whitespaces.
    return(x)
  }
  exclude_redundancy = function(x) {
    # Create a character vector that consists of meaningless word chunks.
    redun <- c("unclassified u.s. department of state case no.", "doc no.", "date:",
             "state dept. - produced to house select benghazi comm. subject to agreement on sensitive information & redactions.", "no foia waiver.", "subject:", "sent:", "monday",
               "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "f-",
               "release in full", "release in part", "state-", "subject", "fvv", "sent", "scb",
             "/", ":", ",")
    # Remove elements in redun from the email set
    x <- tolower(x)
    for (red in redun) {
    x <- gsub(red, "", x)
    }
    # Since we don't need any words more than once.
    x <- toString(unique(unlist(strsplit(x, split = " ", fixed = T))))
    return(x)
  }
  list <- unlist(lapply(lapply(lapply(list, exclude_redundancy), remove_extra), removeWords, stopwords))
  return(gsub("\\s+", " ",list))
}
mth <- c("january", "february", "march", "april", "may", "june", "july", "august",
             "september", "october", "november", "december")
```

###Data-cleaning
```{r}
res_var <- as.numeric(str_extract(HRC[[1]]$content, "[0-9]+"))
HRC_power_feat <- Corpus(VectorSource(clean_pw(HRC[[1]]$content)))
HRC <- Corpus(VectorSource(clean(HRC[[1]]$content)))
```


###Creating Word Features
```{r}
dtm = DocumentTermMatrix(HRC, control = list(stemming=T))
dtm1 = as.data.frame(as.matrix(dtm))
dtm = removeSparseTerms(dtm, .97)
dtm = as.data.frame(as.matrix(dtm))
```

###External Function
```{r}
convert_text_to_sentences = function(text, lang = "en") {
  # Check if the string is only whitespace
  if (text %in% c("", " ") || str_count(text, "\\s") == str_length(text)) {
    return(c())
  }
  sentence_token_annotator = Maxent_Sent_Token_Annotator(language = lang)
  text <- as.String(text)
  sentence.boundaries = NLP::annotate(text, sentence_token_annotator)
  sentences = text[sentence.boundaries]
  return(sentences)
}
```

###Creating Power Features based on sentences
```{r}
power_features_sentence = function(corpus) {
  n = length(corpus)
  power = matrix(NA, nrow = n, ncol = 5, dimnames = list(seq(1, n)))
  hrcs = corpus
  #hrcs = tm_map(corpus, stripWhitespace)
  #hrcs = tm_map(hrcs, stemDocument)
  for (i in 1:n) {

    hrc = hrcs[i]

    text = as.data.frame(hrc)[2]
    sents = convert_text_to_sentences(text)
    sents2 = head(unlist(strsplit(sents, split = ".", fixed = T)), -1)

    # Number of sentences.
    power1 = length(sents2)

    # Average sentence length.
    power2 = sum(stri_count(sents2, regex="\\S+")) / max(1, length(sents2))

    # Number of question marks.
    power3 = sum(str_count(text, fixed("?")))

    #s Number of exclamation marks.
    power4 = sum(str_count(text, fixed("!")))
    
    # Indicator var. sentence_count >= 20
    power5 = 0
    if (power1 >= 20) {
      power5 = 1
    } else {
      power5 = 0
    }

    power[i, ] = c(power1, power2, power3, power4, power5)
    #rownames(power)[i] = title
  }
  rownames(power) = names(hrcs)
  colnames(power) = c("sentence_count", "sentence_avg_length", "question_marks", "exclamation_points", "sentence_count_ind")
  return(power)
}

#This takes approximately 5 minutes
power_feat_sentences <- power_features_sentence(HRC_power_feat)
```

###Creating Power Features based on Words
```{r}
power_features_dtm = function(dtm) {

  new = data.frame("words_count" = c(rep(0,nrow(dtm))), "chars_count" = c(rep(0,nrow(dtm))), "words_avg_length" = c(rep(0,nrow(dtm))), "words_distinct" = c(rep(0,nrow(dtm))), "sd_words" = c(rep(0,nrow(dtm))),"word_diversity" = c(rep(0,nrow(dtm))))
  #colnames(new) = c("words_count","chars_count","words_avg_length","words_distinct","sd_words", "word_diversity")
  words_chars = nchar(colnames(dtm))

  for(i in 1:nrow(dtm)){
    ### power6: total number of words
    new[i,1] = sum(as.numeric(dtm[i,]))

    ### power7: total number of characters
    new[i,2] = as.numeric(t(as.matrix(words_chars))%*%as.matrix(as.numeric(dtm[i,])))

    ### power8: returns the vector of average word length of each txt file
    # Use max so that if there are 0 distinct words we don't try to divide by 0.
    new[i,3] = new[i,2]/max(new[i,1], 1)

    ### power9: number of unique words
    new[i,4] = length(which(as.numeric(dtm[i,])!=0))

    ### power10: standard deviation of word length
    sqrdmean = sum(as.matrix(words_chars^2) * as.matrix(as.numeric(dtm[i,])))/max(new[i,1], 1)
    mean = sum(words_chars*as.matrix(as.numeric(dtm[i,])))/max(new[i,1], 1)
    new[i,5] = sqrdmean-(mean^2)

    ### power11: word diversity
    new[i,6] = new[i,4]/max(new[i,1], 1)

  }
  return(new)
}

pwr_feat_dtm <- power_features_dtm(dtm1) #returns the power feature using words in the emails
```

###Combine word features and power features
```{r}
final_features_train <- cbind(res_var, power_feat_sentences, pwr_feat_dtm, dtm)
write.csv(final_features_train, file = "../../data/train/final_features_train.csv")
```

###Final_features_test
```{r}
# Import & Clean
hrc_test <- Corpus(DirSource('../../data/test', pattern = '.tsv'))
res_test <- as.numeric(str_extract(hrc_test[[1]]$content, "[0-9]+"))
test_power_feat <- Corpus(VectorSource(clean_pw(hrc_test[[1]]$content)))
hrc_test <- Corpus(VectorSource(clean(hrc_test[[1]]$content)))

# Featurize
test_dtm = DocumentTermMatrix(hrc_test, control = list(stemming=T))
test_dtm1 = as.data.frame(as.matrix(test_dtm))
test_dtm = removeSparseTerms(test_dtm, .97)
test_dtm = as.data.frame(as.matrix(test_dtm))
test_pwf_sent <- power_feat_sentences <- power_features_sentence(test_power_feat)
test_pwf_dtm <- power_features_dtm(test_dtm1)

# generate final_features_test.csv
final_features_test <- cbind(res_test, test_pwf_sent, test_pwf_dtm, test_dtm)
write.csv(final_features_test, file = "../../data/test/final_features_test.csv")
```

